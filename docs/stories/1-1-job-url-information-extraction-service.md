# Story 1.1: Job URL Information Extraction Service

Status: in-progress

## Story

As a job seeker,
I want to paste a job posting URL and have the job details automatically extracted,
so that I can save time on manual data entry and quickly capture opportunities.

## Acceptance Criteria

### Given a user has a job posting URL
**When** they provide the URL to the extraction service
**Then** the system should:

1. **AC-1**: Accept URLs from supported platforms (LinkedIn, Indeed, Glassdoor, AngelList)
2. **AC-2**: Extract job title, company name, location, and job description within 10 seconds
3. **AC-3**: Return structured JSON data with extracted fields
4. **AC-4**: Handle extraction failures gracefully with clear error messages
5. **AC-5**: Return partial data if some fields cannot be extracted
6. **AC-6**: Validate URL format before attempting extraction
7. **AC-7**: Implement timeout protection (10-second hard limit per PRD NFR-1.2)

### Edge Cases
- Invalid URL format → Return 400 error with "Invalid URL format" message
- Unsupported platform → Return 400 error listing supported platforms
- Network timeout → Return 408 error with "Request timeout" message
- Parsing failure → Return 200 with partial data and warnings array

## Tasks / Subtasks

### Backend Development

- [ ] **Task 1**: Create URL extraction service infrastructure (AC: #1, #2, #6)
  - [ ] 1.1: Create `internal/services/url_extractor` package
  - [ ] 1.2: Define `ExtractRequest` and `ExtractResponse` structs
  - [ ] 1.3: Implement URL validation (format check, protocol validation)
  - [ ] 1.4: Create platform detector (identify LinkedIn/Indeed/Glassdoor/AngelList from URL)

- [ ] **Task 2**: Implement platform-specific parsers (AC: #2, #3)
  - [ ] 2.1: Create `LinkedInParser` with HTML selector mappings
  - [ ] 2.2: Create `IndeedParser` with HTML selector mappings
  - [ ] 2.3: Create `GlassdoorParser` with HTML selector mappings
  - [ ] 2.4: Create `AngelListParser` with HTML selector mappings
  - [ ] 2.5: Implement generic fallback parser (extract title, meta tags)

- [ ] **Task 3**: Build HTTP client with timeout controls (AC: #7)
  - [ ] 3.1: Configure Go HTTP client with 10-second timeout
  - [ ] 3.2: Implement retry logic (max 2 retries with exponential backoff)
  - [ ] 3.3: Add User-Agent header to prevent bot blocking

- [ ] **Task 4**: Create extraction API endpoint (AC: #3, #4, #5)
  - [ ] 4.1: Create `POST /api/v1/extract/job-url` handler
  - [ ] 4.2: Implement request validation middleware
  - [ ] 4.3: Wire service to handler with error mapping
  - [ ] 4.4: Return structured JSON response with extracted fields + warnings

- [ ] **Task 5**: Error handling and logging (AC: #4)
  - [ ] 5.1: Implement error types (ValidationError, TimeoutError, ParsingError)
  - [ ] 5.2: Add structured logging for extraction attempts (platform, duration, success)
  - [ ] 5.3: Return user-friendly error messages per PRD NFR-3.3

### Testing

- [ ] **Task 6**: Unit tests for extraction service (AC: All)
  - [ ] 6.1: Test URL validation (valid/invalid formats)
  - [ ] 6.2: Test platform detection logic
  - [ ] 6.3: Test each platform parser with mock HTML
  - [ ] 6.4: Test fallback parser behavior
  - [ ] 6.5: Test timeout handling
  - [ ] 6.6: Test partial data extraction scenarios

- [ ] **Task 7**: Integration tests for API endpoint (AC: All)
  - [ ] 7.1: Test successful extraction from all 4 platforms (using test fixtures)
  - [ ] 7.2: Test error cases (invalid URL, unsupported platform, timeout)
  - [ ] 7.3: Test response format validation
  - [ ] 7.4: Test API response times (<500ms per NFR-1.2)

## Dev Notes

### Architecture Constraints

**From Architecture Document:**
- **Technology Stack**: Go 1.23 backend with existing Gin framework [Source: docs/architecture.md#Technology-Stack]
- **Error Handling Pattern**: Use standardized error format `{error, code, details}` [Source: docs/architecture.md#Error-Handling]
- **API Versioning**: All endpoints under `/api/v1/` prefix [Source: docs/architecture.md#API-Structure]
- **Logging**: Use structured logging (logrus/zap) with JSON output [Source: docs/architecture.md#Logging]

**Performance Requirements:**
- API response time target: <500ms (90th percentile) [Source: docs/PRD.md#NFR-1.2]
- Extraction timeout: 10 seconds hard limit [Source: docs/PRD.md#FR-1.2]

**Rate Limiting (from solutioning-gate-check):**
- Implement rate limiting: 30 URL extractions per user per day [Source: docs/PRD.md#NFR-2.5]
- Return 429 Too Many Requests with Retry-After header when limit exceeded

### Project Structure Notes

**Expected File Locations** (per brownfield Go project structure):
```
backend/
├── internal/
│   ├── services/
│   │   └── url_extractor/
│   │       ├── extractor.go          # Main service interface
│   │       ├── parsers/
│   │       │   ├── linkedin.go
│   │       │   ├── indeed.go
│   │       │   ├── glassdoor.go
│   │       │   ├── angellist.go
│   │       │   └── fallback.go        # Generic HTML parser
│   │       └── models.go              # Request/Response structs
│   ├── handlers/
│   │   └── extract_handler.go         # HTTP handler for /api/v1/extract/job-url
│   └── middleware/
│       └── rate_limit.go              # Rate limiting middleware (if not exists)
└── tests/
    └── integration/
        └── extract_test.go            # Integration tests
```

**Naming Conventions:**
- Handlers: `{Feature}Handler` (e.g., `ExtractHandler`)
- Services: `{Feature}Service` (e.g., `URLExtractorService`)
- Models: Use clear struct names (e.g., `ExtractJobURLRequest`, `ExtractJobURLResponse`)

### Implementation Guidance

**Web Scraping Strategy:**
1. **HTTP Client**: Use standard `net/http` with `http.Client{Timeout: 10 * time.Second}`
2. **HTML Parsing**: Use `golang.org/x/net/html` or `github.com/PuerkitoBio/goquery`
3. **Selectors**: CSS selectors for each platform (document in code comments)
4. **Fallback**: Generic parser extracts `<title>`, `<meta name="description">`, OpenGraph tags

**Error Handling Pattern:**
```go
type ErrorResponse struct {
    Error   string   `json:"error"`
    Code    string   `json:"code"`
    Details []string `json:"details,omitempty"`
}
```

**Response Format:**
```go
type ExtractJobURLResponse struct {
    Success   bool              `json:"success"`
    Data      *ExtractedJobData `json:"data,omitempty"`
    Warnings  []string          `json:"warnings,omitempty"`
    Error     *ErrorResponse    `json:"error,omitempty"`
}

type ExtractedJobData struct {
    Title       string `json:"title"`
    Company     string `json:"company"`
    Location    string `json:"location"`
    Description string `json:"description"`
    Platform    string `json:"platform"` // "linkedin" | "indeed" | "glassdoor" | "angellist"
}
```

### Testing Standards

**Unit Test Coverage Target:** 70%+ per NFR-5.3 [Source: docs/PRD.md]

**Test Data:**
- Create mock HTML fixtures for each platform in `testdata/html/`
- Use table-driven tests for parser validation
- Test both happy paths and error scenarios

**Integration Tests:**
- Mock external HTTP calls (do not hit live job sites)
- Test full request/response cycle through Gin handler
- Validate JSON response structure

### Recommended Approach (from solutioning-gate-check)

**From Medium Priority Observation M3:**
- Implement generic HTML parser as fallback [Source: docs/implementation-readiness-report-2025-12-30.md#M3]
- Add monitoring for extraction failure rates (structured logging)
- Consider headless browser (Puppeteer/Playwright) for JavaScript-heavy sites (post-MVP)

### References

- **PRD**: Functional Requirement FR-1.2 (URL Extraction) [Source: docs/PRD.md#FR-1.2]
- **Architecture**: ADR-009 (if exists for scraping approach) or general API patterns [Source: docs/architecture.md]
- **Performance NFR**: NFR-1.2 (API <500ms), NFR-1.4 (extraction <10s) [Source: docs/PRD.md#Non-Functional-Requirements]
- **Error Handling**: NFR-3.3 (user-friendly messages) [Source: docs/PRD.md#NFR-3.3]
- **Rate Limiting**: NFR-2.5 (30 URLs/day) [Source: docs/PRD.md#NFR-2.5]
- **Solutioning Gate Check**: Medium Priority Observations [Source: docs/implementation-readiness-report-2025-12-30.md]

## Dev Agent Record

### Context Reference

- `docs/stories/1-1-job-url-information-extraction-service.context.xml`

### Agent Model Used

<!-- Will be populated during dev-story execution -->

### Debug Log References

<!-- Will be populated during dev-story execution -->

### Completion Notes List

<!-- Will be populated during dev-story execution -->

### File List

<!-- Will be populated during dev-story execution -->

---

## Senior Developer Review (AI)

**Reviewer**: Simon
**Date**: 2026-01-03
**Outcome**: **CHANGES REQUESTED**

### Summary

The URL extraction service implementation demonstrates **solid engineering** with clean architecture, comprehensive error handling, and working LinkedIn and Indeed parsers. All tests pass, and the core extraction functionality works as designed. However, several **medium-priority gaps** prevent production readiness:

1. **Missing rate limiting** (NFR-2.5): No 30 URLs/day per-user enforcement
2. **Missing retry logic** (Task 3.2): HTTP requests have no retry mechanism
3. **No HTML sanitization** (Security constraint): Extracted job descriptions not sanitized for XSS
4. **Missing generic fallback parser** (Task 2.5, M3 observation): No platform-agnostic parser
5. **Low test coverage** (36.9% vs 70% target NFR-5.3)
6. **No API performance tests** (Task 7.4): Response time validation missing

The implementation is **functionally correct** for supported platforms but needs these enhancements before merging to main.

### Acceptance Criteria Coverage

| AC# | Description | Status | Evidence |
|-----|-------------|--------|----------|
| AC-1 | Accept URLs from supported platforms (LinkedIn, Indeed, Glassdoor, AngelList) | ⚠️ **PARTIAL** | ✅ LinkedIn: `extractor.go:75-76`<br>✅ Indeed: `extractor.go:77-78`<br>❌ Glassdoor: Disabled (line 83, documented)<br>❌ AngelList: Disabled (line 80, documented) |
| AC-2 | Extract job title, company name, location, and job description within 10 seconds | ✅ **IMPLEMENTED** | ✅ Title/Company/Location/Description: `parser_linkedin.go:54-58`, `parser_indeed.go:111-114`<br>✅ 10s timeout: `parser.go:54` |
| AC-3 | Return structured JSON data with extracted fields | ✅ **IMPLEMENTED** | ✅ Response struct: `models.go:10-17`<br>✅ JSON handler: `handlers/extract.go:39-43`<br>✅ Standard format: `pkg/response/response.go:9-14` |
| AC-4 | Handle extraction failures gracefully with clear error messages | ✅ **IMPLEMENTED** | ✅ Error handling: `parser.go:72-86`<br>✅ User-friendly messages: `pkg/errors/errors.go`<br>✅ Graceful degradation: `parser_linkedin.go:77-81` |
| AC-5 | Return partial data if some fields cannot be extracted | ✅ **IMPLEMENTED** | ✅ Warnings system: `parser_linkedin.go:62-75`<br>✅ Partial data allowed: Only fails if both title AND company missing<br>✅ Response with warnings: `pkg/response/response.go:58-64` |
| AC-6 | Validate URL format before attempting extraction | ✅ **IMPLEMENTED** | ✅ URL validation: `extractor.go:47-66`<br>✅ Protocol check: line 57-59<br>✅ Host validation: line 61-63 |
| AC-7 | Implement timeout protection (10-second hard limit per PRD NFR-1.2) | ✅ **IMPLEMENTED** | ✅ 10s timeout: `parser.go:54`<br>✅ Context timeout handling: `parser.go:72-74` |

**Summary**: 6 of 7 ACs fully implemented. AC-1 is PARTIAL due to Glassdoor and AngelList being disabled (documented limitation, acceptable for MVP).

### Task Completion Validation

| Task | Marked As | Verified As | Evidence |
|------|-----------|-------------|----------|
| **Task 1**: URL extraction service infrastructure | ❌ Incomplete | ✅ **COMPLETE** | Package, structs, validation, platform detector all exist |
| 1.1: Create package | ❌ | ✅ | `backend/internal/services/urlextractor/` |
| 1.2: Define structs | ❌ | ✅ | `models.go:6-17` |
| 1.3: URL validation | ❌ | ✅ | `extractor.go:47-66` |
| 1.4: Platform detector | ❌ | ✅ | `extractor.go:68-88` |
| **Task 2**: Platform-specific parsers | ❌ Incomplete | ⚠️ **PARTIAL** | 2/4 parsers + no fallback |
| 2.1: LinkedInParser | ❌ | ✅ | `parser_linkedin.go:14-86` |
| 2.2: IndeedParser | ❌ | ✅ | `parser_indeed.go:15-206` |
| 2.3: GlassdoorParser | ❌ | ❌ | Disabled (anti-scraping 403) |
| 2.4: AngelListParser | ❌ | ❌ | Disabled (Cloudflare 403) |
| 2.5: Generic fallback parser | ❌ | ❌ | **NOT IMPLEMENTED** |
| **Task 3**: HTTP client with timeout | ❌ Incomplete | ⚠️ **PARTIAL** | 2/3 subtasks |
| 3.1: 10s timeout | ❌ | ✅ | `parser.go:54` |
| 3.2: Retry logic | ❌ | ❌ | **NOT IMPLEMENTED** |
| 3.3: User-Agent header | ❌ | ✅ | `parser.go:62` |
| **Task 4**: Extraction API endpoint | ❌ Incomplete | ✅ **COMPLETE** | All subtasks verified |
| 4.1: POST handler | ❌ | ✅ | `handlers/extract.go:24-44` |
| 4.2: Request validation | ❌ | ✅ | `handlers/extract.go:27-30` |
| 4.3: Service wiring | ❌ | ✅ | `handlers/extract.go:32-36` |
| 4.4: JSON response | ❌ | ✅ | `handlers/extract.go:39-43` |
| **Task 5**: Error handling and logging | ❌ Incomplete | ✅ **COMPLETE** | All subtasks verified |
| 5.1: Error types | ❌ | ✅ | `pkg/errors/errors.go:37-40` |
| 5.2: Structured logging | ❌ | ✅ | `extractor.go:92-123` |
| 5.3: User-friendly messages | ❌ | ✅ | Throughout codebase |
| **Task 6**: Unit tests | ❌ Incomplete | ⚠️ **PARTIAL** | 5/6 subtasks, 36.9% coverage |
| 6.1: URL validation tests | ❌ | ✅ | `extractor_test.go:13-50` (7 cases) |
| 6.2: Platform detection tests | ❌ | ✅ | `extractor_test.go:52-87` (8 cases) |
| 6.3: Parser tests with mock HTML | ❌ | ⚠️ | LinkedIn & Indeed tested, others N/A |
| 6.4: Fallback parser tests | ❌ | ❌ | No fallback parser exists |
| 6.5: Timeout handling tests | ❌ | ⚠️ | Implicit coverage, no explicit test |
| 6.6: Partial extraction tests | ❌ | ✅ | `parser_linkedin_test.go` |
| **Task 7**: Integration tests | ❌ Incomplete | ⚠️ **PARTIAL** | 3/4 subtasks |
| 7.1: Test all platforms | ❌ | ⚠️ | Only 2/2 active platforms tested |
| 7.2: Test error cases | ❌ | ✅ | `extract_test.go` (3 error tests) |
| 7.3: Response format validation | ❌ | ✅ | All handler tests validate JSON |
| 7.4: API response time tests | ❌ | ❌ | **NOT IMPLEMENTED** |

**Summary**: 14 of 23 subtasks fully verified (60.9%), 4 partial, 5 not done.
**IMPORTANT**: All tasks correctly marked as incomplete `[ ]` - no false completions ✅

### Key Findings

#### HIGH Severity

*None* - All critical functionality works correctly.

#### MEDIUM Severity

1. **[Med] Missing Rate Limiting (NFR-2.5)**
   **Issue**: No per-user rate limiting implemented (30 URLs/day requirement)
   **Impact**: Users can exceed quota, potential abuse/cost overruns
   **Location**: Middleware missing, no rate limit check in `handlers/extract.go` or `routes/extract.go:16`
   **Recommendation**: Implement rate limiting middleware or repository-based tracking

2. **[Med] Missing Retry Logic (Task 3.2)**
   **Issue**: HTTP requests have no retry mechanism (task explicitly requires "max 2 retries with exponential backoff")
   **Impact**: Transient network failures cause immediate errors instead of retrying
   **Location**: `parser.go:70` - `client.Do(req)` has no retry wrapper
   **Recommendation**: Wrap HTTP calls with retry logic (e.g., 2 retries, 1s/2s backoff)

3. **[Med] Missing HTML Sanitization (Security Constraint)**
   **Issue**: Story context constraint requires "Sanitize extracted HTML content before storing (prevent XSS via malicious job descriptions)" - not implemented
   **Impact**: Malicious job postings could inject XSS payloads into descriptions
   **Location**: `parser_linkedin.go:58`, `parser_indeed.go:114` - descriptions not sanitized
   **Recommendation**: Add bluemonday sanitization (per architecture decision) before returning data

4. **[Med] Missing Generic Fallback Parser (Task 2.5, M3)**
   **Issue**: No platform-agnostic fallback parser (extract title/description from meta tags/OpenGraph)
   **Impact**: When platform-specific parsers fail, entire extraction fails instead of falling back
   **Location**: No fallback in `parser.go:38-50` or elsewhere
   **Recommendation**: Implement generic parser using `<title>`, `<meta name="description">`, OpenGraph tags

5. **[Med] Test Coverage Below Target (36.9% vs 70% NFR-5.3)**
   **Issue**: `go test -cover` shows 36.9% coverage, below 70% requirement
   **Impact**: Insufficient test coverage increases regression risk
   **Location**: `backend/internal/services/urlextractor/`
   **Recommendation**: Add tests for error paths, edge cases, timeout scenarios

6. **[Med] Missing API Performance Tests (Task 7.4)**
   **Issue**: No benchmark or performance tests validating <500ms API response time (NFR-1.2)
   **Impact**: Cannot verify performance requirement compliance
   **Location**: No `*_benchmark_test.go` files or performance assertions
   **Recommendation**: Add Go benchmark tests for extraction endpoints

#### LOW Severity

7. **[Low] Task Checkboxes Not Updated**
   **Issue**: All tasks marked as incomplete `[ ]` despite significant work completed
   **Impact**: Difficult to track actual progress
   **Recommendation**: Update story file task checkboxes as work completes

8. **[Low] No Integration with Existing Application Model**
   **Issue**: Extraction service doesn't integrate with `internal/models/application.go` (Story 1.3 dependency)
   **Impact**: None for this story (correct isolation), but note for Story 1.3
   **Location**: Story context mentions this integration point
   **Recommendation**: Document this as expected for Story 1.3

### Test Coverage and Gaps

**Current Coverage**: 36.9% (urlextractor service), 2.1% (handlers package overall)

**Test Quality**: GOOD - Well-structured table-driven tests, clear assertions, proper mocking

**Gaps**:
- ❌ No timeout scenario tests (context cancellation, deadline exceeded)
- ❌ No retry logic tests (doesn't exist)
- ❌ No rate limiting tests (doesn't exist)
- ❌ No HTML sanitization tests (doesn't exist)
- ❌ No fallback parser tests (doesn't exist)
- ❌ No performance/benchmark tests
- ⚠️ Limited error path coverage (network failures, malformed responses)

**Strengths**:
- ✅ URL validation thoroughly tested (7 cases)
- ✅ Platform detection comprehensive (8 cases)
- ✅ Handler tests cover success, warnings, and common errors
- ✅ Parser tests use realistic mock HTML fixtures

### Architectural Alignment

✅ **COMPLIANT** with architecture document:
- ✅ Go 1.24 with Gin framework
- ✅ Handler → Service layering pattern followed
- ✅ Standardized error format (`pkg/errors`, `pkg/response`)
- ✅ `/api/v1/` endpoint prefix (but route is `/api/extract-job-url` not `/api/v1/extract-job-url` - see note below)
- ✅ Structured logging with platform, duration, success tracking
- ✅ 10-second timeout protection

⚠️ **MINOR DEVIATION**:
- Route registered as `/api/extract-job-url` (`routes/extract.go:16`) instead of `/api/v1/extract-job-url`
- Likely registered under `/api` group, so full path may be `/api/extract-job-url` instead of versioned endpoint
- **Recommendation**: Verify route registration in `cmd/server/main.go` - if not under `/api/v1` group, this violates architecture ADR

### Security Notes

1. **[Med] XSS Risk - No HTML Sanitization**
   Malicious job descriptions could contain `<script>` tags or event handlers. Must sanitize before storing/displaying.
   **Fix**: Use bluemonday library (architecture decision) in `parser_linkedin.go:58` and `parser_indeed.go:114`

2. **[Low] SSRF Protection - URL Validation**
   ✅ Good: URL validation prevents `file://`, `localhost`, etc.
   ⚠️ Enhancement: Consider blocking private IP ranges (10.0.0.0/8, 192.168.0.0/16, 127.0.0.0/8)

3. **[Low] User-Agent Spoofing**
   ✅ Good: Browser User-Agent set to avoid bot blocking
   Note: Ethical web scraping - job sites may have ToS restrictions

### Best-Practices and References

**Tech Stack**: Go 1.24, Gin 1.10, goquery 1.11, testify 1.10

**Go Best Practices Applied**:
- ✅ Context propagation for cancellation/timeouts
- ✅ Interface-based design (`Parser`, `HTTPFetcher`)
- ✅ Table-driven tests
- ✅ Error wrapping with context
- ✅ Clear package structure

**References**:
- [Go HTTP Client Best Practices](https://golang.org/pkg/net/http/#Client) - Timeout configuration ✅
- [Effective Go](https://golang.org/doc/effective_go) - Error handling, interfaces ✅
- [goquery Documentation](https://github.com/PuerkitoBio/goquery) - CSS selector patterns ✅
- [bluemonday](https://github.com/microcosm-cc/bluemonday) - HTML sanitization (recommended but not implemented)

### Action Items

#### Code Changes Required

- [ ] [High] Implement per-user rate limiting (30 URLs/day) [file: backend/internal/middleware/rate_limit.go (create), routes/extract.go:16]
- [ ] [High] Add HTML sanitization using bluemonday before returning extracted descriptions [file: backend/internal/services/urlextractor/parser_linkedin.go:58, parser_indeed.go:114]
- [ ] [High] Implement HTTP retry logic (2 retries, exponential backoff) [file: backend/internal/services/urlextractor/parser.go:70-77]
- [ ] [Med] Implement generic fallback parser (extract from meta tags/OpenGraph) [file: backend/internal/services/urlextractor/parser_fallback.go (create)]
- [ ] [Med] Add test coverage to reach 70% target (add timeout tests, error path tests, edge cases) [file: backend/internal/services/urlextractor/*_test.go]
- [ ] [Med] Add API performance benchmark tests (<500ms validation) [file: backend/internal/handlers/extract_benchmark_test.go (create)]
- [ ] [Low] Verify API route is under `/api/v1/` group (architecture requirement) [file: backend/cmd/server/main.go:53, routes/extract.go:12-17]
- [ ] [Low] Update task checkboxes in story file to reflect completed work [file: docs/stories/1-1-job-url-information-extraction-service.md:35-78]

#### Advisory Notes

- Note: Glassdoor and AngelList disabled due to anti-scraping (403 errors) - documented in README, acceptable for MVP
- Note: Test fixtures in `testdata/` are well-organized - maintain this pattern for future parsers
- Note: LinkedIn guest API is clever approach - avoids auth requirements while being stable
- Note: Indeed's JSON-LD + HTML fallback strategy is robust - good engineering
- Note: Consider circuit breaker pattern for production (repeated failures → temporary disable platform)
- Note: Consider adding Prometheus metrics for extraction success/failure rates by platform
